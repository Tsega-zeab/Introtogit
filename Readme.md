# Transistors
transistors are the very core of today’s technology. It is a three terminal device using two closely spaced point contacts on a wafer of germanium.it is a three terminal device and consists of three distinct layers. Two of them are doped to give one type of semiconductor and there is the opposite type, i.e. two may be n-type and one p-type, or two may be p-type and one may be n-type... They are arranged so that the two similar layers of the transistor sandwich the layer of the opposite type. As a result these semiconductor devices are designated as either PNP transistors or NPN transistors according to the way they are made up. The names of the three electrodes are : 
## Base: 
The base of the transistor gains its name from the fact that in early transistors, this electrode formed the base for the whole device. The earliest point contact transistors had two point contacts Placed onto the base material. This base material formed the base connection . . . and the name stuck. 
## Emitter:
The emitter gains its name from the fact that it emits the charge carriers
## Collector:
The collector gains its name from the fact that it collects the charge carriers.
# Moore's law 

Is the observation that the number of transistors in a dense integrated circuit (IC) doubles every two years. Moore's law is an observation and projection of a historical trend, Rather than a law of physics. It is an empirical relationship linked to gains from experience in production.The law claims that we can expect the speed and capability of our computers to increase every two years because of this, yet we will pay less for them. Another tenet of Moore's Law asserts that this growth is exponential. The law is attributed to Gordon Moore, the co-founder and former CEO of Intel. As transistors in integrated circuits become more efficient, computers become smaller and faster. Chips and transistors are microscopic structures that contain carbon and silicon molecules, which are aligned perfectly to move electricity along the circuit faster. The faster a microchip processes electrical signals, the more efficient a computer becomes. The cost of higher-powered computers has been dropping annually, partly because of lower labor costs and reduced semiconductor prices.
# The Lambda calculus  

is an abstract mathematical theory of computation, involving \lambda functions. The lambda calculus can be thought of as the theoretical foundation of functional programming. It is a Turing complete language; that is to say, any machine which can compute the lambda calculus can compute everything a Turing machine can (and vice versa).The \lambdaλ notation is based on function abstraction and application based on variable binding and substitution.
# How Does A computer work

Computer is a digital information-processing machine, works by changing information into binary numbers (ones and zeros) and then using simple mathematics to make  decisions about how to rearrange those numbers into words or actions. A digital system stores and operates on information in a very specific way by storing information in a bit (or multiple collections of bits). A bit is a variable that can have only one of two values: it can either be a 1, or be a 0.                                    A computer treats any type of information (not only numbers but also letters, words, dates) as if it consisted simply of binary ones and zeros. For example, a computer can translate the letter “A” typed into its keyboard into a string of ones and zeros, such as 1000001. One reason for this is that once it is in a binary form, the information can be stored and moved about more easily. On a hard disc, the “ones” could be stored as magnetized spots on the disc, while the zeroes can be stored as un-magnetized spots. Once information has been converted to ones and zeros, the computer can get to work. That the computer’s functions are based on the movement.                                                                                                                                                       Computer functions are based on the movement and transformation of electrical pulses (representing ones and zeroes) in electrical circuits. Inside the computer are electrical circuits that decode the zeros and ones, by adding and subtracting them. These circuits are called the logic of the computer, because the calculations they make are similar to simple logic decisions. For example, if you press the A key on the computer keyboard, circuits inside the computer receive pulses of electricity representing the A in binary form - 1000001. Those pulses are sent to logic circuits that make yes or no decisions based on the input they receive. A very simple example would be a circuit that determines whether the input send to it is a one or a zero. The output of the circuit is a new piece of information - a binary one or zero that is the result of the simple yes or no decision.
# Turing Machine

A Turing machine is a mathematical model of computation describing an abstract machine that manipulates symbols on a strip of tape according to a table of rules. Despite the model's simplicity, it is capable of implementing any computer algorithm.
The machine operates on an infinite memory tape divided into discrete cells, each of which can hold a single symbol drawn from a finite set of symbols called the alphabet of the machine. It has a "head" that, at any point in the machine's operation, is positioned over one of these cells, and a "state" selected from a finite set of states. At each step of its operation, the head reads the symbol in its cell. Then, based on the symbol and the machine's own present state, the machine writes a symbol into the same cell, and moves the head one step to the left or the right, or halts the computation. The choice of which replacement symbol to write and which direction to move is based on a finite table that specifies what to do for each combination of the current state and the symbol that is read.
The Turing machine was invented in 1936 by Alan Turing, who called it an "a-machine" (automatic machine).It was Turing's Doctoral advisor, Alonzo Church, who later coined the term "Turing machine" in a review.
Turing machines proved the existence of fundamental limitations on the power of mechanical computation.While they can express arbitrary computations, their minimalist design makes them unsuitable for computation in practice: real-world computers are based on different designs that, unlike Turing machines, use random-access memory. 
    ### ARPANET was one of the first general-purpose computer networks. It connected time-sharing computers at government-supported research sites, principally universities in the United States, and it soon became a critical piece of infrastructure for the computer science research community in the United States. Tools and applications—such as the simple mail transfer protocol (SMTP, commonly referred to as e-mail), for sending short messages, and the file transfer protocol (FTP), for longer transmissions—quickly emerged. In order to achieve cost-effective interactive communications between computers, which typically communicate in short bursts of data, ARPANET employed the new technology of packet switching. Packet switching takes large messages (or chunks of computer data) and breaks them into smaller, manageable pieces (known as packets) that can travel independently over any available circuit to the target destination, where the pieces are reassembled. Thus, unlike traditional voice communications, packet switching does not require a single dedicated circuit between each pair of users.
    # Internet and WWW
    
